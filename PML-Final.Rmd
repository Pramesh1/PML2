---
title: "Practical Machine Learning- Assignment"
author: "P.Rogbeer"
date: "Saturday, September 6, 2014"
output: html_document
---

#### Executive Summary:
The goal of this exercise is to build a model to automatically predict how people perform some predetermined types of exercises. The model is : built using a **training data set** , cross validated with a **testing data set** and finally, executed only  once with a 3rd **test data set**.An **accuracy of 99%** was achieved with the model built, making it a reliable predicting tool in the current case.

#### I. Load libraries and data  
Preload all libraries and have a sneak preview of the data set 
 
```{r,warning=FALSE,message=FALSE,options(cache = TRUE)}
library(caret)
library(ggplot2)
library(randomForest)
library(e1071)
set.seed(1234)
data_trn <- read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
data_tst <- read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
data_training<-data_trn 
data_testing<-data_tst 

```


####II.  Exploratory Data Analysis - (Streamlining)  

Once libraries and data loaded an dout of the way, the next step was to perform a measure of Exploratory Data Analysis to clean the data sets, identify variables, get information on data types, and so on.  
It was noted that  
- The dependent variable (**Classe**) is of type '*factor*' with 5 levels  
- There seems to be a lot of non available data (**NA**)  

The next phase was then to clean and prepare the data sets for computation. 


##### 1. Removing columns with "NA" and irrelevant variables (Column 1:7)


```{r,warning=FALSE, message=FALSE}

#TRAINING Data Set
data_train <- data_training[ , colSums(is.na(data_training)) == 0]  # Keep columns where the sum of NA is Zero
data_train<- data_train[,-(1:7)] # Get rid of columns 1-7

#TEST Data Set
data_test <- data_testing[ , colSums(is.na(data_testing)) == 0]  
data_test<- data_test[,-(1:7)]
                    
```

##### 2. Make the nearzero diagnosis

To further see if there are more candidates for removel , perform the "Nearzero" test.
(ref http://tgmstat.wordpress.com/2014/03/06/near-zero-variance-predictors/)

```{r,warning=FALSE, message=FALSE}

nzv_train<- nearZeroVar(data_train, saveMetrics = TRUE)
nzv_train[nzv_train$nzv, ]
                    
```

This diagnosis proposed **no candidate**(variable) for removal.  


The **PCA analysis** was also performed but later dropped from this assignment.  
I had an unknown programming problem which was taking too much time to solving.

##### 3. Remove highly correlated variables

Last step in the EDA was to get rid of the highly correlated variables.
There is a lot of discussion on this theme . One author (N.R.Draper and H.smith "Applied regression Analysis") notes that *".. the existence of intercorrelation may be an obstacle for interpretation"*.

```{r,warning=FALSE,  message=FALSE}

cor_train <- findCorrelation(cor(data_train[,-53]), cutoff = 0.75)
data_train<-data_train[,-cor_train]
data_test<-data_test[,-cor_train]

```


####III.  Split the Data set

The training data set **(data_training)** set was split in 2 :  
 - one set will used for training the model  
 - the 2nd set used to crossvalidate the model

A default 60/40 split was performed.


```{r, message=FALSE, results='hide'}
# split the training data sets into 2 on a 60/40 basis
#
Sp_Index <- createDataPartition(y=data_train$classe, p=0.6, list=FALSE)   
training<- data_train[Sp_Index,]     # Put the 60% in this new data set.
testing<- data_train[-Sp_Index,]    # Put the remaining 40% in this new data set.

```


####IV.  Apply predicting with Random Forest

The **CARET** package was used to perform this *Random Forest* processing.

```{r,warning=FALSE,  message=FALSE}

ModFit <- randomForest(classe ~ ., data = training, importance = TRUE, ntrees = 500)
varImpPlot(ModFit, cex = 0.7)                         
                    
```



**Gini** is defined as *"inequity"* or a measure of *"node impurity"* in tree-based classification.(ref. Course materila)  
A **low Gini** (i.e. higher decrease in Gini) means that a particular predictor variable plays a **greater** role in partitioning the data into the defined classes.
There is a clear visualisation of the **"important"** variables.


####V.  Cross validation and Accuracy testing

The **confusion matrix** was the applied to both the `*training*` and `*test*` set to check the **accuracy** of our model.

Result of the verification :  
 - Training set : **100% accurate** as expected  
 - Testing set : **>99% accurate**

The model has a good accuracy and can be confidently used.


```{r,warning=FALSE,  message=FALSE}
crossval<-predict(ModFit, training)
confusionMatrix(training$classe, predict(ModFit, training))

```

```{r,warning=FALSE,  message=FALSE}

confusionMatrix(testing$classe, predict(ModFit, testing))

```

####VI.  The 20 files

 
 
```{r,warning=FALSE,  message=FALSE}

Result <- predict(ModFit,data_test[,-33])
answers<-Result[1:20]
answers

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)

```

 



 